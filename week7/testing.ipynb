{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from time import sleep\n",
    "import itertools\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".envrc\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "URLS = {\n",
    "    \"hist_data\": \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/historical/\",\n",
    "    \"recent_data\": \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/recent/\",\n",
    "}\n",
    "\n",
    "# for category, url in URLS.items():\n",
    "#     page = requests.get(url)\n",
    "#     soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "#     links = soup.find_all(\"a\", href=re.compile(\".pdf$|.txt$|.zip$\"))\n",
    "\n",
    "#     for link in links:\n",
    "#         path = Path(\"./data/zips\") / category\n",
    "#         path.mkdir(parents=True, exist_ok=True)\n",
    "#         file_path = path / link[\"href\"]\n",
    "#         mode = \"w+b\" if \"pdf\" or \"zip\" in link[\"href\"] else \"w+\"\n",
    "#         file_url = url + link[\"href\"]\n",
    "#         if not file_path.is_file():\n",
    "#             with requests.get(file_url) as r:\n",
    "#                 with open(str(file_path), mode) as f:\n",
    "#                     f.write(r.content)\n",
    "#                     sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in URLS:\n",
    "    source_path = Path(\"./data\") / category / \"zips\"\n",
    "    target_path = Path(\"./data\") / category / \"metadata\"\n",
    "    target_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file in source_path.glob(\"*.zip\"):\n",
    "        with ZipFile(file, \"r\") as zip:\n",
    "            fname_lst = [fname for fname in zip.namelist() if \"Metadaten\" in fname] # produkt_klima_tag for data, Metadaten for metadata\n",
    "            for fname in fname_lst:\n",
    "                zip.extract(fname, target_path)\n",
    "    \n",
    "\n",
    "files_to_rem = target_path.glob(\"*.html\")\n",
    "for file in files_to_rem:\n",
    "    file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_rem = Path(\"./data/hist_data/metadata/\").glob(\"*.html\")\n",
    "for file in files_to_rem:\n",
    "    file.unlink()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes= {\n",
    "#     'STATIONS_ID': \"string\", \n",
    "#     'QN_3': \"UInt8\", \n",
    "#     'QN_4': \"UInt8\", \n",
    "#     ' RSK': \"Float32\", \n",
    "#     'RSKF': \"UInt8\", \n",
    "#     'SHK_TAG': \"UInt8\",\n",
    "#     '  NM': \"string\", \n",
    "#     ' TMK': \"Float32\", \n",
    "#     ' UPM': \"Float32\", \n",
    "#     ' TXK': \"Float32\", \n",
    "#     ' TNK': \"Float32\", \n",
    "#     ' TGK': \"Float32\",\n",
    "# }\n",
    "\n",
    "\n",
    "# df_new_lst = [pd.read_table(\n",
    "#         file, \n",
    "#         sep=\";\",\n",
    "#         usecols=['STATIONS_ID', 'MESS_DATUM', 'QN_3', 'QN_4', ' RSK',\n",
    "#         'RSKF', 'SHK_TAG', '  NM', ' TMK', ' UPM',\n",
    "#         ' TXK', ' TNK', ' TGK'],\n",
    "#         dtype_backend=\"pyarrow\",\n",
    "#         dtype=dtypes,\n",
    "#         na_values=None,\n",
    "#     ) for file in Path(\"./data/recent_data/extracts\").glob(\"*.txt\")]\n",
    "\n",
    "# df_hist_lst = [pd.read_table(\n",
    "#         file, \n",
    "#         sep=\";\",\n",
    "#         usecols=['STATIONS_ID', 'MESS_DATUM', 'QN_3', 'QN_4', ' RSK',\n",
    "#         'RSKF', 'SHK_TAG', '  NM', ' TMK', ' UPM',\n",
    "#         ' TXK', ' TNK', ' TGK'],\n",
    "#         dtype_backend=\"pyarrow\",\n",
    "#         dtype=dtypes,\n",
    "#         na_values=None,\n",
    "#     ) for file in Path(\"./data/hist_data/extracts\").glob(\"*.txt\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat(df_new_lst + df_hist_lst)\n",
    "# df = df.replace(-999, None).astype(dtypes)\n",
    "# df[\"MESS_DATUM\"] = pd.to_datetime(df[\"MESS_DATUM\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\", nonexistent=\"NaT\")\n",
    "# df.columns = df.columns.str.replace(' ', '')\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet(\"./data/parquet/data.parquet\")\n",
    "# df = pd.read_parquet(\"./data/parquet/data.parquet\", dtype_backend=\"pyarrow\")\n",
    "# df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes = {\n",
    "#     'Stations_id': \"string\",\n",
    "#     'Stationshoehe': \"Float32\",\n",
    "#     'Geogr.Breite': \"Float32\",\n",
    "#     'Geogr.Laenge': \"Float32\",\n",
    "#     'von_datum': \"string\",\n",
    "#     'bis_datum': \"string\",\n",
    "#     'Stationsname': \"string\",\n",
    "# }\n",
    "\n",
    "# df_new_lst = [pd.read_table(\n",
    "#         file, \n",
    "#         sep=\";\",\n",
    "#         encoding=\"latin1\",\n",
    "#         dtype=dtypes,\n",
    "#         dtype_backend=\"pyarrow\",\n",
    "#         na_values=None) for file in Path(\"./data/recent_data/metadata\").glob(\"Metadaten_Geographie*\")]\n",
    "\n",
    "# df_hist_lst = [pd.read_table(\n",
    "#         file, \n",
    "#         sep=\";\",\n",
    "#         encoding=\"latin1\",\n",
    "#         dtype=dtypes,\n",
    "#         dtype_backend=\"pyarrow\",\n",
    "#         na_values=None) for file in Path(\"./data/hist_data/metadata\").glob(\"Metadaten_Geographie*\")]\n",
    "\n",
    "# df = pd.concat(df_new_lst + df_hist_lst)\n",
    "# df.Stations_id = df.Stations_id.str.replace(' ', '')\n",
    "# df = df.replace(-999, None).astype(dtypes)\n",
    "# df[\"von_datum\"] = pd.to_datetime(df[\"von_datum\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\")\n",
    "# df[\"bis_datum\"] = pd.to_datetime(df[\"bis_datum\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\")\n",
    "# df.columns = df.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet(\"./data/parquet/metadata_geo.parquet\")\n",
    "# df = pd.read_parquet(\"./data/parquet/metadata_geo.parquet\", dtype_backend=\"pyarrow\")\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new_lst = [pd.read_table(\n",
    "#         file, \n",
    "#         sep=\";\",\n",
    "#         encoding=\"latin1\",\n",
    "#         dtype_backend=\"pyarrow\",\n",
    "#         na_values=None) for file in Path(\"./data/recent_data/metadata\").glob(\"Metadaten_Stationsname_Betreibername*\")]\n",
    "# df_lst = []\n",
    "# for df in df_new_lst:\n",
    "#         split_idx = df.loc[df['Stationsname']==\"Betreibername\"].index[0]\n",
    "#         df.columns = [\"stations_id\", \"betreibername\", \"betrieb_von_datum\", \"betrieb_bis_datum\"]\n",
    "#         df = df.iloc[split_idx+1:-1]\n",
    "#         df_lst.append(df)\n",
    "\n",
    "# df = pd.concat(df_lst).reset_index(drop=True)\n",
    "# df.stations_id = df.stations_id.str.replace(' ', '')\n",
    "# df[\"betrieb_von_datum\"] = pd.to_datetime(df[\"betrieb_von_datum\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\")\n",
    "# df[\"betrieb_bis_datum\"] = pd.to_datetime(df[\"betrieb_bis_datum\"], format=\"%Y%m%d\", errors=\"coerce\", utc=False).dt.tz_localize(\"Europe/Brussels\", ambiguous = \"NaT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet(\"./data/parquet/metadata_operator.parquet\")\n",
    "# df = pd.read_parquet(\"./data/parquet/metadata_operator.parquet\", dtype_backend=\"pyarrow\")\n",
    "# df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# import pyspark.sql.types as T\n",
    "# import pyspark.sql.functions as F\n",
    "\n",
    "# spark.conf.set('spark.sql.legacy.parquet.datetimeRebaseModeInRead', 'CORRECTED')\n",
    "\n",
    "# schema = T.StructType([\n",
    "#     T.StructField('STATIONS_ID', T.StringType(), False),\n",
    "#     T.StructField('MESS_DATUM', T.TimestampType(), True),\n",
    "#     T.StructField('QN_3', T.ShortType(), True),\n",
    "#     T.StructField('QN_4', T.ShortType(), True),\n",
    "#     T.StructField('RSK', T.DoubleType(), True),\n",
    "#     T.StructField('RSKF', T.ShortType(), True),\n",
    "#     T.StructField('SHK_TAG', T.IntegerType(), True),\n",
    "#     T.StructField('NM', T.StringType(), True),\n",
    "#     T.StructField('TMK', T.DoubleType(), True),\n",
    "#     T.StructField('UPM', T.DoubleType(), True),\n",
    "#     T.StructField('TXK', T.DoubleType(), True),\n",
    "#     T.StructField('TNK', T.DoubleType(), True),\n",
    "#     T.StructField('TGK', T.DoubleType(), True),\n",
    "# ])\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .appName('SparkNotebook') \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = Path(\"./data/data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .parquet(str(data_path))\n",
    "\n",
    "# df.createOrReplaceTempView(\"temps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = spark.sql(\"\"\"\n",
    "# SELECT distinct(extract(year from MESS_DATUM)) from temps\n",
    "# where date(MESS_DATUM) >= \"1900-01-01\"\n",
    "# order by 1 desc\n",
    "# \"\"\")\n",
    "# results.show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = spark.sql(\"\"\"\n",
    "# SELECT STATIONS_ID,extract(year from MESS_DATUM), max(TXK) as max_temp_2m from temps\n",
    "# group by 1,2\n",
    "# order by 3 desc\n",
    "# \"\"\")\n",
    "# results.show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week7--DKIWNv2-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
