{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from time import sleep\n",
    "import itertools\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".envrc\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "URLS = {\n",
    "    \"hist_data\": \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/historical/\",\n",
    "    \"recent_data\": \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/recent/\",\n",
    "}\n",
    "\n",
    "# for category, url in URLS.items():\n",
    "#     page = requests.get(url)\n",
    "#     soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "#     links = soup.find_all(\"a\", href=re.compile(\".pdf$|.txt$|.zip$\"))\n",
    "\n",
    "#     for link in links:\n",
    "#         path = Path(\"./data/zips\") / category\n",
    "#         path.mkdir(parents=True, exist_ok=True)\n",
    "#         file_path = path / link[\"href\"]\n",
    "#         mode = \"w+b\" if \"pdf\" or \"zip\" in link[\"href\"] else \"w+\"\n",
    "#         file_url = url + link[\"href\"]\n",
    "#         if not file_path.is_file():\n",
    "#             with requests.get(file_url) as r:\n",
    "#                 with open(str(file_path), mode) as f:\n",
    "#                     f.write(r.content)\n",
    "#                     sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in URLS:\n",
    "    source_path = Path(\"./data\") / category / \"zips\"\n",
    "    target_path = Path(\"./data\") / category / \"metadata\"\n",
    "    target_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file in source_path.glob(\"*.zip\"):\n",
    "        with ZipFile(file, \"r\") as zip:\n",
    "            fname_lst = [fname for fname in zip.namelist() if \"Metadaten\" in fname] # produkt_klima_tag for data, Metadaten for metadata\n",
    "            for fname in fname_lst:\n",
    "                zip.extract(fname, target_path)\n",
    "    \n",
    "\n",
    "files_to_rem = target_path.glob(\"*.html\")\n",
    "for file in files_to_rem:\n",
    "    file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_rem = Path(\"./data/hist_data/metadata/\").glob(\"*.html\")\n",
    "for file in files_to_rem:\n",
    "    file.unlink()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# path = Path(\"./data/\") / \"recent_data\" / \"extracts\"\n",
    "\n",
    "# dtypes= {\n",
    "#     'STATIONS_ID': \"string\", \n",
    "#     'QN_3': \"UInt8\", \n",
    "#     'QN_4': \"UInt8\", \n",
    "#     ' RSK': \"Float32\", \n",
    "#     'RSKF': \"UInt8\", \n",
    "#     'SHK_TAG': \"UInt8\",\n",
    "#     '  NM': \"string\", \n",
    "#     ' TMK': \"Float32\", \n",
    "#     ' UPM': \"Float32\", \n",
    "#     ' TXK': \"Float32\", \n",
    "#     ' TNK': \"Float32\", \n",
    "#     ' TGK': \"Float32\",\n",
    "# }\n",
    "\n",
    "\n",
    "# df_new_lst = [pd.read_table(\n",
    "#         file, \n",
    "#         sep=\";\",\n",
    "#         usecols=['STATIONS_ID', 'MESS_DATUM', 'QN_3', 'QN_4', ' RSK',\n",
    "#         'RSKF', 'SHK_TAG', '  NM', ' TMK', ' UPM',\n",
    "#         ' TXK', ' TNK', ' TGK'],\n",
    "#         dtype_backend=\"pyarrow\",\n",
    "#         dtype=dtypes,\n",
    "#         na_values=None,\n",
    "#     ) for file in Path(\"./data/recent_data/extracts\").glob(\"*.txt\")]\n",
    "\n",
    "# df_hist_lst = [pd.read_table(\n",
    "#         file, \n",
    "#         sep=\";\",\n",
    "#         usecols=['STATIONS_ID', 'MESS_DATUM', 'QN_3', 'QN_4', ' RSK',\n",
    "#         'RSKF', 'SHK_TAG', '  NM', ' TMK', ' UPM',\n",
    "#         ' TXK', ' TNK', ' TGK'],\n",
    "#         dtype_backend=\"pyarrow\",\n",
    "#         dtype=dtypes,\n",
    "#         na_values=None,\n",
    "#     ) for file in Path(\"./data/hist_data/extracts\").glob(\"*.txt\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat(df_new_lst + df_hist_lst)\n",
    "# df.MESS_DATUM = pd.to_datetime(df.MESS_DATUM, format=\"%Y%m%d\")\n",
    "# df = df.replace(-999, None).astype(dtypes)\n",
    "# df.columns = df.columns.str.replace(' ', '')\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1781, 1, 1, 0, 0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.to_parquet(\"./data/data.parquet\")\n",
    "# df = pd.read_parquet(\"./data/data.parquet\", dtype_backend=\"pyarrow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6489 entries, 0 to 1\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Stations_id    6489 non-null   string \n",
      " 1   Stationshoehe  6489 non-null   Float32\n",
      " 2   Geogr.Breite   6489 non-null   Float32\n",
      " 3   Geogr.Laenge   6489 non-null   Float32\n",
      " 4   von_datum      6489 non-null   string \n",
      " 5   bis_datum      6489 non-null   string \n",
      " 6   Stationsname   6489 non-null   string \n",
      "dtypes: Float32(3), string(4)\n",
      "memory usage: 348.5 KB\n"
     ]
    }
   ],
   "source": [
    "dtypes = {\n",
    "    'Stations_id': \"string\",\n",
    "    'Stationshoehe': \"Float32\",\n",
    "    'Geogr.Breite': \"Float32\",\n",
    "    'Geogr.Laenge': \"Float32\",\n",
    "    'von_datum': \"string\",\n",
    "    'bis_datum': \"string\",\n",
    "    'Stationsname': \"string\",\n",
    "}\n",
    "\n",
    "df_new_lst = [pd.read_table(\n",
    "        file, \n",
    "        sep=\";\",\n",
    "        encoding=\"latin1\",\n",
    "        dtype=dtypes,\n",
    "        dtype_backend=\"pyarrow\",\n",
    "        na_values=None) for file in Path(\"./data/recent_data/metadata\").glob(\"Metadaten_Geographie*\")]\n",
    "\n",
    "df_hist_lst = [pd.read_table(\n",
    "        file, \n",
    "        sep=\";\",\n",
    "        encoding=\"latin1\",\n",
    "        dtype=dtypes,\n",
    "        dtype_backend=\"pyarrow\",\n",
    "        na_values=None) for file in Path(\"./data/hist_data/metadata\").glob(\"Metadaten_Geographie*\")]\n",
    "\n",
    "df = pd.concat(df_new_lst + df_hist_lst)\n",
    "# df[\"Von_Datum\"] = pd.to_datetime(df[\"Von_Datum\"], format=\"mixed\", dayfirst=True)\n",
    "# df[\"Bis_Datum\"] = pd.to_datetime(df[\"Bis_Datum\"], format=\"mixed\", dayfirst=True)\n",
    "df.Stations_id = df.Stations_id.str.replace(' ', '')\n",
    "df = df.replace(-999, None).astype(dtypes)\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet(\"./data/parquet/metadata_geo.parquet\")\n",
    "# df = pd.read_parquet(\"./data/metadata_geo.parquet\", dtype_backend=\"pyarrow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/28 13:05:31 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInRead' instead.\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# import pyspark.sql.types as T\n",
    "# import pyspark.sql.functions as F\n",
    "\n",
    "# spark.conf.set('spark.sql.legacy.parquet.datetimeRebaseModeInRead', 'CORRECTED')\n",
    "\n",
    "# schema = T.StructType([\n",
    "#     T.StructField('STATIONS_ID', T.StringType(), False),\n",
    "#     T.StructField('MESS_DATUM', T.TimestampType(), True),\n",
    "#     T.StructField('QN_3', T.ShortType(), True),\n",
    "#     T.StructField('QN_4', T.ShortType(), True),\n",
    "#     T.StructField('RSK', T.DoubleType(), True),\n",
    "#     T.StructField('RSKF', T.ShortType(), True),\n",
    "#     T.StructField('SHK_TAG', T.IntegerType(), True),\n",
    "#     T.StructField('NM', T.StringType(), True),\n",
    "#     T.StructField('TMK', T.DoubleType(), True),\n",
    "#     T.StructField('UPM', T.DoubleType(), True),\n",
    "#     T.StructField('TXK', T.DoubleType(), True),\n",
    "#     T.StructField('TNK', T.DoubleType(), True),\n",
    "#     T.StructField('TGK', T.DoubleType(), True),\n",
    "# ])\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .appName('SparkNotebook') \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = Path(\"./data/data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .parquet(str(data_path))\n",
    "\n",
    "# df.createOrReplaceTempView(\"temps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: string (nullable = true)\n",
      " |-- MESS_DATUM: timestamp (nullable = true)\n",
      " |-- QN_3: short (nullable = true)\n",
      " |-- QN_4: short (nullable = true)\n",
      " |-- RSK: float (nullable = true)\n",
      " |-- RSKF: short (nullable = true)\n",
      " |-- SHK_TAG: short (nullable = true)\n",
      " |-- NM: string (nullable = true)\n",
      " |-- TMK: float (nullable = true)\n",
      " |-- UPM: float (nullable = true)\n",
      " |-- TXK: float (nullable = true)\n",
      " |-- TNK: float (nullable = true)\n",
      " |-- TGK: float (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|extract(year FROM MESS_DATUM)|\n",
      "+-----------------------------+\n",
      "|2023                         |\n",
      "|2022                         |\n",
      "|2021                         |\n",
      "|2020                         |\n",
      "|2019                         |\n",
      "|2018                         |\n",
      "|2017                         |\n",
      "|2016                         |\n",
      "|2015                         |\n",
      "|2014                         |\n",
      "|2013                         |\n",
      "|2012                         |\n",
      "|2011                         |\n",
      "|2010                         |\n",
      "|2009                         |\n",
      "|2008                         |\n",
      "|2007                         |\n",
      "|2006                         |\n",
      "|2005                         |\n",
      "|2004                         |\n",
      "|2003                         |\n",
      "|2002                         |\n",
      "|2001                         |\n",
      "|2000                         |\n",
      "|1999                         |\n",
      "|1998                         |\n",
      "|1997                         |\n",
      "|1996                         |\n",
      "|1995                         |\n",
      "|1994                         |\n",
      "|1993                         |\n",
      "|1992                         |\n",
      "|1991                         |\n",
      "|1990                         |\n",
      "|1989                         |\n",
      "|1988                         |\n",
      "|1987                         |\n",
      "|1986                         |\n",
      "|1985                         |\n",
      "|1984                         |\n",
      "+-----------------------------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# results = spark.sql(\"\"\"\n",
    "# SELECT distinct(extract(year from MESS_DATUM)) from temps\n",
    "# where date(MESS_DATUM) >= \"1900-01-01\"\n",
    "# order by 1 desc\n",
    "# \"\"\")\n",
    "# results.show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------+-----------+\n",
      "|STATIONS_ID|extract(year FROM MESS_DATUM)|max_temp_2m|\n",
      "+-----------+-----------------------------+-----------+\n",
      "|       5064|2019                         |41.2       |\n",
      "|      13670|2019                         |41.2       |\n",
      "|       2968|2019                         |41.1       |\n",
      "|        603|2019                         |40.9       |\n",
      "|       2629|2019                         |40.9       |\n",
      "|       1078|2019                         |40.7       |\n",
      "|       1327|2019                         |40.6       |\n",
      "|       5100|2019                         |40.6       |\n",
      "|      13696|2019                         |40.5       |\n",
      "|        161|2019                         |40.4       |\n",
      "|       3490|2019                         |40.4       |\n",
      "|       3257|2022                         |40.3       |\n",
      "|       2667|2019                         |40.3       |\n",
      "|       2600|2015                         |40.3       |\n",
      "|       5717|2019                         |40.2       |\n",
      "|       1443|2003                         |40.2       |\n",
      "|       1424|2019                         |40.2       |\n",
      "|       3257|2015                         |40.2       |\n",
      "|       2522|2003                         |40.2       |\n",
      "|       3545|2019                         |40.2       |\n",
      "|       1981|2022                         |40.1       |\n",
      "|       3442|2019                         |40.1       |\n",
      "|       1327|2003                         |40.1       |\n",
      "|       1420|2019                         |40.1       |\n",
      "|       6217|2019                         |40.0       |\n",
      "|       1303|2019                         |40.0       |\n",
      "|       4411|2019                         |40.0       |\n",
      "|       1766|2019                         |40.0       |\n",
      "|       2700|1983                         |40.0       |\n",
      "|        294|2022                         |40.0       |\n",
      "|       3903|2003                         |40.0       |\n",
      "|       6266|2022                         |40.0       |\n",
      "|       2480|2019                         |40.0       |\n",
      "|       6093|2022                         |39.9       |\n",
      "|       5099|2019                         |39.9       |\n",
      "|       2110|2019                         |39.9       |\n",
      "|       3904|2019                         |39.9       |\n",
      "|       7341|2019                         |39.9       |\n",
      "|       2480|2015                         |39.8       |\n",
      "|       5146|2022                         |39.8       |\n",
      "+-----------+-----------------------------+-----------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# results = spark.sql(\"\"\"\n",
    "# SELECT STATIONS_ID,extract(year from MESS_DATUM), max(TXK) as max_temp_2m from temps\n",
    "# group by 1,2\n",
    "# order by 3 desc\n",
    "# \"\"\")\n",
    "# results.show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week7--DKIWNv2-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
